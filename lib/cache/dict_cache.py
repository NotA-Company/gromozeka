"""
Thread-safe dictionary-based cache with TTL and size limits, dood!

Implements CacheInterface with generic type support, pluggable key generation,
thread safety, TTL expiration, and LRU-like eviction.

Features:
    - Generic type support (CacheInterface[K, V])
    - Pluggable KeyGenerator strategy
    - Thread-safe operations with RLock
    - Configurable TTL for entries
    - Maximum size enforcement with eviction
    - Automatic cleanup of expired entries
    - Cache statistics and monitoring

Example:
    from lib.cache import DictCache, StringKeyGenerator

    cache = DictCache[str, str](
        keyGenerator=StringKeyGenerator(),
        defaultTtl=3600,  # 1 hour
        maxSize=1000      # 1000 entries max
    )

    await cache.set("key", "value")
    result = await cache.get("key")
"""

import logging
import threading
import time
from typing import Any, Dict, Optional, Tuple

from .interface import CacheInterface
from .key_generator import KeyGenerator
from .types import K, V

logger = logging.getLogger(__name__)


class DictCache(CacheInterface[K, V]):
    """
    Thread-safe dictionary-based cache with TTL and size limits, dood!

    Stores entries as (value, timestamp) tuples with string keys generated by
    KeyGenerator. Uses RLock for thread safety and LRU-like eviction for size
    management.

    Type Parameters:
        K: Key type (supported by KeyGenerator)
        V: Value type (any storable type)

    Example:
        cache = DictCache[str, str](
            keyGenerator=StringKeyGenerator(),
            defaultTtl=3600
        )
    """

    def __init__(
        self,
        keyGenerator: KeyGenerator[K],
        defaultTtl: int = 3600,
        maxSize: Optional[int] = 1000,
    ):
        """
        Initialize cache with configuration, dood!

        Args:
            keyGenerator: Strategy for converting keys to strings
            defaultTtl: Default TTL in seconds (default: 3600)
                0 = immediate expiration, negative = never expires
            maxSize: Maximum cache entries (default: 1000)
                None = unlimited size

        Raises:
            ValueError: If maxSize is not None and not positive
        """
        if maxSize is not None and maxSize <= 0:
            raise ValueError(f"maxSize must be a positive integer or None, got {maxSize}, dood!")

        self._cache: Dict[str, Tuple[V, float]] = {}
        self._keyGenerator = keyGenerator
        self._defaultTtl = defaultTtl
        self._maxSize = maxSize
        self._lock = threading.RLock()

    def _isExpired(self, timestamp: float, ttl: int) -> bool:
        """
        Check if a cache entry has expired, dood!

        Args:
            timestamp: Unix timestamp when entry was stored
            ttl: TTL in seconds (0 = always expired, negative = never expires)

        Returns:
            bool: True if entry has exceeded its TTL
        """
        if ttl == 0:
            return True  # Always expired
        if ttl < 0:
            return False  # Never expired
        return time.time() - timestamp > ttl

    def _cleanupExpired(self) -> None:
        """
        Remove all expired entries from the cache, dood!

        Note:
            - Must be called within lock context
            - Complexity: O(n) where n is cache size
        """
        expiredKeys = [
            key for key, (_, timestamp) in self._cache.items() if self._isExpired(timestamp, self._defaultTtl)
        ]
        for key in expiredKeys:
            del self._cache[key]

        if expiredKeys:
            logger.debug(f"Cleaned up {len(expiredKeys)} expired entries, dood!")

    def _evictOldest(self) -> None:
        """
        Enforce maximum cache size through LRU-like eviction, dood!

        Removes oldest entries when cache exceeds maxSize. Sorts by timestamp
        (oldest first) then by key for deterministic behavior.

        Note:
            - Must be called within lock context
            - Complexity: O(n log n) due to sorting
        """
        if self._maxSize is None or len(self._cache) <= self._maxSize:
            return

        # Sort by timestamp (oldest first), if timestamps are equal, sort by key
        sortedItems = sorted(self._cache.items(), key=lambda item: (item[1][1], item[0]))  # timestamp then key

        excessCount = len(self._cache) - self._maxSize
        for i in range(excessCount):
            key = sortedItems[i][0]
            del self._cache[key]

        logger.debug(f"Removed {excessCount} oldest entries to enforce size limit, dood!")

    async def get(self, key: K, ttl: Optional[int] = None) -> Optional[V]:
        """
        Get cached value by key, dood!

        Retrieves value if exists and hasn't expired. Returns None if key not
        found or expired. Automatically triggers cleanup of expired entries.

        Args:
            key: Cache key to retrieve
            ttl: Optional TTL override in seconds
                None = use default, 0 = always expired, negative = never expires

        Returns:
            Optional[V]: Cached value if found and not expired, None otherwise
        """
        try:
            effectiveTtl = ttl if ttl is not None else self._defaultTtl

            with self._lock:
                self._cleanupExpired()

                keyStr = self._keyGenerator.generateKey(key)

                if keyStr in self._cache:
                    value, timestamp = self._cache[keyStr]
                    if not self._isExpired(timestamp, effectiveTtl):
                        logger.debug(f"Cache hit for key: {keyStr}, dood!")
                        return value
                    else:
                        # Remove expired entry
                        del self._cache[keyStr]
                        logger.debug(f"Removed expired entry: {keyStr}, dood!")

                logger.debug(f"Cache miss for key: {keyStr}, dood!")
                return None
        except Exception as e:
            logger.error(f"Failed to get cache entry: {e}, dood!")
            return None

    async def set(self, key: K, value: V) -> bool:
        """
        Store value in cache, dood!

        Stores value with current timestamp for TTL calculation. Automatically
        triggers cleanup and enforces size limits through eviction.

        Args:
            key: Cache key to store value under
            value: Value to cache (any storable type)

        Returns:
            bool: True if value was successfully stored, False otherwise
        """
        try:
            with self._lock:
                self._cleanupExpired()

                keyStr = self._keyGenerator.generateKey(key)

                # Add new entry first
                self._cache[keyStr] = (value, time.time())

                # Then enforce size limit
                self._evictOldest()

                logger.debug(f"Stored value for key: {keyStr}, dood!")
                return True

        except Exception as e:
            logger.error(f"Failed to set cache entry: {e}, dood!")
            return False

    def clear(self) -> None:
        """
        Clear all cached data, dood!

        Removes all entries from cache, resetting to empty state. Useful for
        testing or when complete cache reset is required.
        """

        with self._lock:
            self._cache.clear()
            logger.debug("Cleared all cache data, dood!")

    def getStats(self) -> Dict[str, Any]:
        """
        Get cache statistics, dood!

        Returns:
            Dict[str, Any]: Statistics containing:
                - entries (int): Current number of valid entries
                - maxSize (Optional[int]): Maximum cache size limit
                - defaultTtl (int): Default TTL in seconds
                - threadSafe (bool): Whether thread safety is enabled
        """

        with self._lock:
            self._cleanupExpired()
            return {
                "entries": len(self._cache),
                "maxSize": self._maxSize,
                "defaultTtl": self._defaultTtl,
                "threadSafe": self._lock is not None,
            }
